# ğŸ¯ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Calibration Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø§ Backtest

## ğŸ“‹ Ù…Ù‚Ø¯Ù…Ù‡

Ø§ÛŒÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ Ù„ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ **Ø¨Ø§ÛŒØ¯** ÛŒØ§ **Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯** Ø¨Ø§ backtest Ú©Ø§Ù„ÛŒØ¨Ø±Ù‡ Ø´ÙˆÙ†Ø¯ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

**âš ï¸ Ù‡Ø´Ø¯Ø§Ø± Ù…Ù‡Ù…:** Over-fitting Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø¯ Ø¯Ø± live trading Ø´ÙˆØ¯. Ù‡Ù…ÛŒØ´Ù‡:
- Ø§Ø² validation set Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
- Walk-forward analysis Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯
- Ø¯Ø± market regimes Ù…Ø®ØªÙ„Ù ØªØ³Øª Ú©Ù†ÛŒØ¯

---

## ğŸ“Š Table of Contents

1. [Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ (Priority 1)](#1-Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ-Ø­ÛŒØ§ØªÛŒ-priority-1)
2. [Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù… (Priority 2)](#2-Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ-Ù…Ù‡Ù…-priority-2)
3. [Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Priority 3)](#3-Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ-Ù¾ÛŒØ´Ø±ÙØªÙ‡-priority-3)
4. [Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ ØªØºÛŒÛŒØ± Ú©Ù†Ù†Ø¯](#4-Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ-Ú©Ù‡-Ù†Ø¨Ø§ÛŒØ¯-ØªØºÛŒÛŒØ±-Ú©Ù†Ù†Ø¯)
5. [Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Calibration](#5-Ø±ÙˆØ´-Ù‡Ø§ÛŒ-calibration)
6. [Backtest Strategy](#6-backtest-strategy)
7. [Walk-Forward Analysis](#7-walk-forward-analysis)
8. [Optimization Tips](#8-optimization-tips)

---

## 1ï¸âƒ£ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ (Priority 1)

Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ **Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ±** Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ø¯Ø§Ø±Ù†Ø¯ Ùˆ **Ø­ØªÙ…Ø§Ù‹** Ø¨Ø§ÛŒØ¯ calibrate Ø´ÙˆÙ†Ø¯.

### 1.1 â­ Per-Timeframe Slope Thresholds (TrendAnalyzer)

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > trend_detection > slope_thresholds`

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:**
```yaml
slope_thresholds:
  '5m': 0.15    # 15%
  '15m': 0.12   # 12%
  '1h': 0.10    # 10%
  '4h': 0.08    # 8%
```

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ØªØ´Ø®ÛŒØµ trend Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯
- threshold Ø¨Ø§Ù„Ø§ â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ù…Ø§ Ù‚ÙˆÛŒâ€ŒØªØ±
- threshold Ù¾Ø§ÛŒÛŒÙ† â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ù…Ø§ Ø¶Ø¹ÛŒÙâ€ŒØªØ±

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
slope_ranges = {
    '5m': [0.10, 0.12, 0.15, 0.18, 0.20],   # ØªØ³Øª 5 Ù…Ù‚Ø¯Ø§Ø±
    '15m': [0.08, 0.10, 0.12, 0.14, 0.16],
    '1h': [0.06, 0.08, 0.10, 0.12, 0.14],
    '4h': [0.05, 0.06, 0.08, 0.10, 0.12]
}

# Metric to optimize
# - Win rate
# - Profit factor
# - Sharpe ratio
# - Max drawdown
```

**Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
- Timeframeâ€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (5m, 15m): threshold Ø¨Ø§Ù„Ø§ØªØ±
- Timeframeâ€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ (4h): threshold Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
- Ù‡Ø± Ø§Ø±Ø² Ù…Ù…Ú©Ù† Ø§Ø³Øª threshold Ù…ØªÙØ§ÙˆØªÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯

**âš ï¸ Ø®Ø·Ø± Over-fitting:**
- Ù‡Ø± Ø§Ø±Ø² Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ optimize **Ù†Ú©Ù†ÛŒØ¯** (Ø§Ø­ØªÙ…Ø§Ù„ over-fitting Ø¨Ø§Ù„Ø§)
- ÛŒÚ© threshold global Ø¨Ø±Ø§ÛŒ Ù‡Ø± TF Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø±ÛŒØª Ø§Ø±Ø²Ù‡Ø§ Ú©Ø§Ø± Ú©Ù†Ø¯

---

### 1.2 â­ Direction Determination Margin

**Ù…Ú©Ø§Ù†:** `config.yaml > multi_timeframe > direction_margin`

**Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ:** `1.3` (30% margin)

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ LONG/SHORT ØµØ§Ø¯Ø± Ø´ÙˆØ¯
- margin Ø¨Ø§Ù„Ø§ â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ù…Ø§ Ú©Ù…ØªØ±
- margin Ù¾Ø§ÛŒÛŒÙ† â†’ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ù…Ø§ Ø¶Ø¹ÛŒÙâ€ŒØªØ±

**Ø±ÙˆØ´ Calibration:**
```python
# Test range
margins = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6]
# 1.1 = 10% margin (Ù…Ø§Ù†Ù†Ø¯ OLD)
# 1.3 = 30% margin (ÙØ¹Ù„ÛŒ NEW)
# 1.5 = 50% margin (Ø¨Ø³ÛŒØ§Ø± Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡)

# For each margin:
# - Count signals
# - Calculate win rate
# - Calculate profit factor
# - Calculate max drawdown

# Goal: Best balance between quantity and quality
```

**Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
- Market trending: margin Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± (1.1-1.2)
- Market choppy/ranging: margin Ø¨Ø§Ù„Ø§ØªØ± (1.4-1.5)
- **Ø¨Ù‡ØªØ±ÛŒÙ† Ø±Ø§Ù‡:** Adaptive margin Ø¨Ø± Ø§Ø³Ø§Ø³ market regime

**ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡:**
```yaml
# Adaptive margin based on regime
direction_margin:
  trending: 1.2      # Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± trend Ø¯Ø§Ø±
  ranging: 1.5       # Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± range
  volatile: 1.4      # Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± volatile
  default: 1.3       # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
```

---

### 1.3 â­ Minimum Signal Score

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > minimum_signal_score`

**Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ:** `180.0`

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- Ø­Ø¯Ø§Ù‚Ù„ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ØµØ¯ÙˆØ± Ø³ÛŒÚ¯Ù†Ø§Ù„
- ØªØ£Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø± ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ú©ÛŒÙÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§

**Ø±ÙˆØ´ Calibration:**
```python
# Test range
min_scores = [140, 150, 160, 170, 180, 190, 200, 210, 220]

# Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ù‚Ø¯Ø§Ø±:
# 1. ØªØ¹Ø¯Ø§Ø¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
# 2. Win rate
# 3. Average profit per trade
# 4. Max drawdown
# 5. Sharpe ratio

# Goal: Sweet spot between quantity and quality
```

**Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
```
Signal Count vs Min Score

Count
  ^
  |  â—â—â—â—
  |      â—â—â—â—
  |          â—â—â—â—
  |              â—â—â—
  |                  â—â—â—
  +----------------------> Min Score
    140  160  180  200  220

Win Rate vs Min Score

Rate
  ^
  |                  â—â—â—
  |              â—â—â—
  |          â—â—â—
  |      â—â—â—
  |  â—â—â—
  +----------------------> Min Score
    140  160  180  200  220

# Sweet spot: Ø­Ø¯ÙˆØ¯ 170-190
```

---

### 1.4 â­ Timeframe Weights

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > timeframe_weights`

**Ù…Ù‚Ø§Ø¯ÛŒØ± ÙØ¹Ù„ÛŒ:**
```yaml
timeframe_weights:
  '5m': 0.7
  '15m': 0.85
  '1h': 1.0
  '4h': 1.1   # Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø² 1.2
```

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù‡Ø± TF Ú†Ù‚Ø¯Ø± Ø¯Ø± final score ØªØ£Ø«ÛŒØ± Ø¯Ø§Ø±Ø¯
- ØªØ£Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø± direction Ùˆ score Ù†Ù‡Ø§ÛŒÛŒ

**Ø±ÙˆØ´ Calibration:**
```python
# Approach 1: Grid Search
weights_4h = [0.9, 1.0, 1.1, 1.2, 1.3]
weights_5m = [0.6, 0.7, 0.8, 0.9]

# Approach 2: Ratio-based
# Ù‡Ù…Ù‡ weights Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ 1h ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯
base = 1.0  # 1h (reference)
ratios = {
    '5m': [0.6, 0.7, 0.8],
    '15m': [0.8, 0.85, 0.9],
    '4h': [1.0, 1.1, 1.2, 1.3]
}

# Test all combinations
# Metric: Weighted win rate across all TFs
```

**ğŸ’¡ Approach 3: Market Regime Adaptive**
```yaml
# Ø¯Ø± market trendingØŒ HTF Ù…Ù‡Ù…â€ŒØªØ± Ø§Ø³Øª
trending:
  '4h': 1.3
  '1h': 1.0
  '15m': 0.8
  '5m': 0.6

# Ø¯Ø± market choppyØŒ LTF Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
ranging:
  '4h': 0.9
  '1h': 1.0
  '15m': 1.0
  '5m': 0.9
```

---

### 1.5 â­ Risk/Reward Ratios

**Ù…Ú©Ø§Ù†:** `config.yaml > risk`

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:**
```yaml
risk:
  default_stop_loss_percent: 2.0        # SL Ù¾ÛŒØ´â€ŒÙØ±Ø¶
  preferred_risk_reward_ratio: 2.0      # RR ØªØ±Ø¬ÛŒØ­ÛŒ
  min_risk_reward_ratio: 1.5            # Ø­Ø¯Ø§Ù‚Ù„ RR
  atr_trailing_multiplier: 2.0          # Ø¶Ø±ÛŒØ¨ ATR
```

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- ØªØ¹ÛŒÛŒÙ† SL/TP Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø³ÙˆØ¯/Ø¶Ø±Ø± Ù‡Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡
- ØªØ£Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø± profit factor

**Ø±ÙˆØ´ Calibration:**
```python
# Test combinations
sl_percents = [1.5, 2.0, 2.5, 3.0]
rr_ratios = [1.5, 2.0, 2.5, 3.0]
atr_multipliers = [1.5, 2.0, 2.5, 3.0]

# Ø¨Ø±Ø§ÛŒ Ù‡Ø± combination:
for sl in sl_percents:
    for rr in rr_ratios:
        for atr_mult in atr_multipliers:
            backtest_with_params(sl, rr, atr_mult)
            calculate_metrics()

# Metrics:
# - Win rate (Ú©Ø§Ù‡Ø´ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ RR)
# - Average win/loss ratio
# - Profit factor
# - Max consecutive losses
```

**Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
```
# Market Trending
- SL: 2.5-3.0% (Ø¨Ø²Ø±Ú¯ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù† ÙØ¶Ø§ Ø¨Ù‡ trend)
- RR: 2.5-3.0 (target Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ØªØ±)

# Market Ranging
- SL: 1.5-2.0% (Ú©ÙˆÚ†Ú©ØªØ± Ø¨Ø±Ø§ÛŒ Ø­ÙØ§Ø¸Øª)
- RR: 1.5-2.0 (target Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹â€ŒØ¨ÛŒÙ†Ø§Ù†Ù‡)
```

---

### 1.6 â­ Circuit Breaker Thresholds

**Ù…Ú©Ø§Ù†:** `config.yaml > circuit_breaker`

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:**
```yaml
circuit_breaker:
  max_consecutive_losses: 3      # ØªØ¹Ø¯Ø§Ø¯ Ø¶Ø±Ø± Ù…ØªÙˆØ§Ù„ÛŒ
  max_daily_losses_r: 5.0        # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¶Ø±Ø± Ø±ÙˆØ²Ø§Ù†Ù‡ (R)
  cool_down_period_minutes: 60   # Ù…Ø¯Øª ØªÙˆÙ‚Ù
```

**Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ**
- Ù…Ø­Ø§ÙØ¸Øª Ø§Ø² Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± ØºÛŒØ±Ø¹Ø§Ø¯ÛŒ
- Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¶Ø±Ø±Ù‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ†

**Ø±ÙˆØ´ Calibration:**
```python
# Simulate on historical data
consecutive_losses = [2, 3, 4, 5]
daily_loss_r = [3.0, 4.0, 5.0, 6.0, 7.0]
cool_down_periods = [30, 60, 90, 120]  # minutes

# Ø¨Ø±Ø§ÛŒ Ù‡Ø± combination:
# 1. Count circuit breaker triggers
# 2. Analyze if they prevented losses
# 3. Analyze if they missed good opportunities
# 4. Calculate ROI with/without circuit breaker

# Goal: Minimize false positives while catching real crashes
```

**ğŸ’¡ Historical Analysis:**
```python
# Ø¨Ø±Ø±Ø³ÛŒ crash Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ú¯Ø°Ø´ØªÙ‡
crashes = [
    '2021-05-19',  # China ban
    '2022-06-18',  # UST/LUNA collapse
    '2022-11-09',  # FTX collapse
]

# Ø¢ÛŒØ§ circuit breaker Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ trigger Ù…ÛŒâ€ŒØ´Ø¯ØŸ
# Ø¢ÛŒØ§ Ø¶Ø±Ø±Ù‡Ø§ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ø§Ø¯ØŸ
```

---

## 2ï¸âƒ£ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù… (Priority 2)

Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ØªØ£Ø«ÛŒØ± Ù…ØªÙˆØ³Ø· Ø¯Ø§Ø±Ù†Ø¯ Ø§Ù…Ø§ calibration Ø¢Ù†Ù‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‡Ø¯.

### 2.1 ğŸ“Š Momentum Thresholds

**Ù…Ú©Ø§Ù†:** `signal_generation/analyzers/momentum_analyzer.py`

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:**
```python
# Ø®Ø· 70-75 ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹
MOMENTUM_THRESHOLDS = {
    'strong': 0.6,     # 60% of bars in one direction
    'moderate': 0.45,  # 45%
    'weak': 0.3        # 30%
}
```

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
strong_thresholds = [0.55, 0.60, 0.65, 0.70]
moderate_thresholds = [0.40, 0.45, 0.50]
weak_thresholds = [0.25, 0.30, 0.35]

# Per timeframe calibration
MOMENTUM_THRESHOLDS_PER_TF = {
    '5m': {'strong': 0.65, 'moderate': 0.50, 'weak': 0.35},
    '15m': {'strong': 0.62, 'moderate': 0.47, 'weak': 0.32},
    '1h': {'strong': 0.60, 'moderate': 0.45, 'weak': 0.30},
    '4h': {'strong': 0.58, 'moderate': 0.43, 'weak': 0.28}
}
```

---

### 2.2 ğŸ“Š MACD Market Type Strengths

**Ù…Ú©Ø§Ù†:** `signal_generation/multi_tf_aggregator.py:78-85`

**Ù…Ù‚Ø§Ø¯ÛŒØ± ÙØ¹Ù„ÛŒ:**
```python
MACD_TYPE_STRENGTH = {
    'A': 1.2,  # A_ types (strong bullish) +20%
    'C': 1.2,  # C_ types (strong bearish) +20%
    'B': 1.0,  # B_ types (neutral)
    'D': 1.0,  # D_ types (neutral)
    'X': 0.8   # X_ types (transition) -20%
}
```

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
a_c_strengths = [1.1, 1.15, 1.2, 1.25, 1.3]
x_strengths = [0.6, 0.7, 0.8, 0.9]

# Analyze historical data:
# Ø¨Ø±Ø§ÛŒ Ù‡Ø± market typeØŒ win rate Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ
for market_type in ['A', 'B', 'C', 'D', 'X']:
    signals = filter_signals_by_macd_type(market_type)
    win_rate = calculate_win_rate(signals)
    # Ø§Ú¯Ø± win rate Ø¨Ø§Ù„Ø§Ø³ØªØŒ strength Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
```

**ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯:**
```python
# Separate bullish and bearish
MACD_TYPE_STRENGTH = {
    # Bullish types
    'A1': 1.3, 'A2': 1.25, 'A3': 1.2,
    'B1': 1.1, 'B2': 1.0, 'B3': 0.9,

    # Bearish types
    'C1': 1.3, 'C2': 1.25, 'C3': 1.2,
    'D1': 1.1, 'D2': 1.0, 'D3': 0.9,

    # Transition
    'X1': 0.9, 'X2': 0.8, 'X3': 0.7
}
```

---

### 2.3 ğŸ“Š Pattern Scores Per Timeframe

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > pattern_recognition > pattern_scores`

**Ù…Ù‚Ø§Ø¯ÛŒØ± ÙØ¹Ù„ÛŒ:**
```yaml
pattern_scores:
  '5m': 8
  '15m': 12
  '1h': 15
  '4h': 20
```

**Ø±ÙˆØ´ Calibration:**
```python
# Analyze pattern effectiveness per TF
for tf in ['5m', '15m', '1h', '4h']:
    patterns_in_tf = get_patterns_in_timeframe(tf)

    # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù„Ú¯Ùˆ:
    for pattern in patterns_in_tf:
        win_rate = calculate_pattern_win_rate(pattern, tf)
        avg_profit = calculate_pattern_avg_profit(pattern, tf)

    # Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¯Ø± ÛŒÚ© TF Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ score Ø¢Ù† TF Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯

# Test ranges
scores = {
    '5m': [6, 8, 10, 12],
    '15m': [10, 12, 14, 16],
    '1h': [13, 15, 17, 19],
    '4h': [18, 20, 22, 24]
}
```

**ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡:**
```yaml
# Per-pattern scores per TF
pattern_scores_advanced:
  '5m':
    'engulfing': 10
    'hammer': 8
    'doji': 5
    # ...
  '1h':
    'engulfing': 18
    'hammer': 15
    'doji': 10
```

---

### 2.4 ğŸ“Š Phase Multipliers

**Ù…Ú©Ø§Ù†:** `signal_generation/multi_tf_aggregator.py:68-76`

**Ù…Ù‚Ø§Ø¯ÛŒØ± ÙØ¹Ù„ÛŒ:**
```python
PHASE_MULTIPLIERS = {
    'early': 1.2,       # +20% - Best opportunity
    'developing': 1.1,  # +10%
    'mature': 0.9,      # -10% - Caution
    'late': 0.7,        # -30% - Risky
    'pullback': 1.1,    # +10%
    'transition': 0.8,  # -20%
    'undefined': 1.0    # No change
}
```

**Ø±ÙˆØ´ Calibration:**
```python
# Historical analysis
for phase in PHASE_MULTIPLIERS.keys():
    signals_in_phase = filter_signals_by_phase(phase)
    win_rate = calculate_win_rate(signals_in_phase)
    avg_rr = calculate_avg_rr(signals_in_phase)

    # Ø§Ú¯Ø± win rate Ø¨Ø§Ù„Ø§ â†’ multiplier Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
    # Ø§Ú¯Ø± win rate Ù¾Ø§ÛŒÛŒÙ† â†’ multiplier Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒØ¯

# Test ranges
early_mults = [1.1, 1.15, 1.2, 1.25, 1.3]
late_mults = [0.5, 0.6, 0.7, 0.8]
```

---

### 2.5 ğŸ“Š Confidence Score Thresholds

**Ù…Ú©Ø§Ù†:** `signal_generation/systems/confidence_calculator.py` (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹)

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ:**
```yaml
confidence_thresholds:
  very_high: 0.85    # Ø¨Ø³ÛŒØ§Ø± Ù…Ø·Ù…Ø¦Ù†
  high: 0.75         # Ù…Ø·Ù…Ø¦Ù†
  medium: 0.60       # Ù…ØªÙˆØ³Ø·
  low: 0.45          # Ú©Ù…

min_confidence_to_trade: 0.60  # Ø­Ø¯Ø§Ù‚Ù„ confidence Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ù‡
```

**Ø±ÙˆØ´ Calibration:**
```python
# Backtest Ø¨Ø§ confidence filtering
min_confidences = [0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75]

for min_conf in min_confidences:
    signals = filter_signals_by_confidence(min_conf)

    metrics = {
        'count': len(signals),
        'win_rate': calculate_win_rate(signals),
        'profit_factor': calculate_profit_factor(signals),
        'sharpe': calculate_sharpe(signals)
    }

# Goal: Ø¨Ù‡ØªØ±ÛŒÙ† balance Ø¨ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ú©ÛŒÙÛŒØª
```

**Ù†ØªÛŒØ¬Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
```
Confidence vs Win Rate

Win %
  ^
  |                      â—â—â—
  |                  â—â—â—
  |              â—â—â—
  |          â—â—â—
  |      â—â—â—
  |  â—â—â—
  +-----------------------> Min Confidence
    0.45  0.55  0.65  0.75  0.85

# Sweet spot: Ø­Ø¯ÙˆØ¯ 0.60-0.65
```

---

### 2.6 ğŸ“Š Correlation Safety Thresholds

**Ù…Ú©Ø§Ù†:** `config.yaml > correlation_management`

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙØ¹Ù„ÛŒ:**
```yaml
correlation_management:
  correlation_threshold: 0.7        # Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
  max_exposure_per_group: 3         # Ø­Ø¯Ø§Ú©Ø«Ø± Ù¾ÙˆØ²ÛŒØ´Ù† Ø¯Ø± Ú¯Ø±ÙˆÙ‡
```

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
corr_thresholds = [0.6, 0.65, 0.7, 0.75, 0.8]
max_exposures = [2, 3, 4, 5]

# Ø¨Ø±Ø§ÛŒ Ù‡Ø± combination:
for threshold in corr_thresholds:
    for max_exp in max_exposures:
        # ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡
        # ØªØ¹Ø¯Ø§Ø¯ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ correlation
        # ØªØ£Ø«ÛŒØ± Ø¨Ø± drawdown
        # ØªØ£Ø«ÛŒØ± Ø¨Ø± diversification

# Goal: Ú©Ø§Ù‡Ø´ drawdown Ø¨Ø¯ÙˆÙ† Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¨
```

---

## 3ï¸âƒ£ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ (Priority 3)

Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ØªØ£Ø«ÛŒØ± Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ø§Ù…Ø§ fine-tuning Ø¢Ù†Ù‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªÙØ§ÙˆØª Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯.

### 3.1 ğŸ”¬ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ÛŒ ÙØ¹Ø§Ù„

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > pattern_recognition > enabled_patterns`

**Ø±ÙˆØ´ Calibration:**
```python
# Test each pattern individually
patterns = [
    'engulfing', 'hammer', 'shooting_star', 'doji',
    'morning_star', 'evening_star', 'three_white_soldiers',
    # ... all 16 patterns
]

for pattern in patterns:
    # ÙÙ‚Ø· Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
    signals = backtest_with_pattern(pattern)

    metrics = {
        'frequency': len(signals),
        'win_rate': calculate_win_rate(signals),
        'avg_profit': calculate_avg_profit(signals),
        'false_signals': count_false_signals(signals)
    }

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒÛŒ Ú©Ù‡ win rate < 45% Ø±Ø§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒÛŒ Ú©Ù‡ win rate > 60% Ø±Ø§ score Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ø¯Ù‡ÛŒØ¯
```

---

### 3.2 ğŸ”¬ Minimum Pattern Quality

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > pattern_recognition > min_pattern_quality`

**Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ:** `0.7`

**Ø±ÙˆØ´ Calibration:**
```python
# Test range
min_qualities = [0.5, 0.6, 0.7, 0.8, 0.9]

for min_qual in min_qualities:
    signals = filter_patterns_by_quality(min_qual)

    # ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡
    # Win rate of remaining patterns
    # Average profit

# Higher quality â†’ fewer but better patterns
```

---

### 3.3 ğŸ”¬ Divergence Lookback Period

**Ù…Ú©Ø§Ù†:** `config.yaml > signal_generation > momentum_analysis > divergence_lookback`

**Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ:** `5`

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
lookbacks = [3, 4, 5, 6, 7, 8, 10]

for lookback in lookbacks:
    divergences = detect_divergences(lookback)

    # ØªØ¹Ø¯Ø§Ø¯ divergence Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
    # Win rate divergence signals
    # False positives

# Shorter lookback â†’ more signals, more noise
# Longer lookback â†’ fewer signals, more reliable
```

---

### 3.4 ğŸ”¬ Anomaly Score Thresholds

**Ù…Ú©Ø§Ù†:** `signal_generation/systems/emergency_circuit_breaker.py`

**Thresholds ÙØ¹Ù„ÛŒ:**
```python
# Volume spike
vol_ratio > 3  # Ø­Ø¬Ù… 3 Ø¨Ø±Ø§Ø¨Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†

# Price change
price_change_pct > 3  # ØªØºÛŒÛŒØ± Ù‚ÛŒÙ…Øª Ø¨ÛŒØ´ Ø§Ø² 3%

# High-Low range
hl_ratio > typical_hl * 2  # 2 Ø¨Ø±Ø§Ø¨Ø± Ù…Ø¹Ù…ÙˆÙ„
```

**Ø±ÙˆØ´ Calibration:**
```python
# Test thresholds
vol_thresholds = [2.5, 3.0, 3.5, 4.0]
price_thresholds = [2.0, 2.5, 3.0, 3.5, 4.0]
hl_multipliers = [1.5, 2.0, 2.5, 3.0]

# Analyze historical anomalies
anomalies = detect_historical_anomalies()

for anomaly in anomalies:
    # Ø¢ÛŒØ§ ÙˆØ§Ù‚Ø¹Ø§Ù‹ ÛŒÚ© crash Ø¨ÙˆØ¯ØŸ
    # Ø¢ÛŒØ§ circuit breaker Ø¨Ø§ÛŒØ¯ trigger Ù…ÛŒâ€ŒØ´Ø¯ØŸ
    # Ø¢ÛŒØ§ false positive Ø¨ÙˆØ¯ØŸ
```

---

### 3.5 ğŸ”¬ ATR Period

**Ù…Ú©Ø§Ù†:** Ù…Ø®ØªÙ„Ù (ATR calculation Ù‡Ø§)

**Ù…Ù‚Ø¯Ø§Ø± ÙØ¹Ù„ÛŒ:** `14` (Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯)

**Ø±ÙˆØ´ Calibration:**
```python
# Test ranges
atr_periods = [10, 12, 14, 16, 18, 20]

for period in atr_periods:
    # ØªØ£Ø«ÛŒØ± Ø¨Ø± SL/TP calculations
    # ØªØ£Ø«ÛŒØ± Ø¨Ø± volatility detection
    # ØªØ£Ø«ÛŒØ± Ø¨Ø± anomaly detection

# Shorter period â†’ more reactive
# Longer period â†’ smoother, less noise
```

---

## 4ï¸âƒ£ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ ØªØºÛŒÛŒØ± Ú©Ù†Ù†Ø¯

Ø§ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ **Ø§ØµÙˆÙ„ Ù…Ù†Ø·Ù‚** Ø³ÛŒØ³ØªÙ… Ù‡Ø³ØªÙ†Ø¯ Ùˆ ØªØºÛŒÛŒØ± Ø¢Ù†Ù‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø³ÛŒØ³ØªÙ… Ø±Ø§ Ø®Ø±Ø§Ø¨ Ú©Ù†Ø¯.

### âŒ Ù†Ø¨Ø§ÛŒØ¯ ØªØºÛŒÛŒØ± Ú©Ù†Ù†Ø¯:

1. **5-candle lookback** Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ divergence
   - Ø§ÛŒÙ† ÛŒÚ© Ø§ØµÙ„ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ ØªØ§ 5 Ú©Ù†Ø¯Ù„ Ù‚Ø¨Ù„ Ù…Ø¹ØªØ¨Ø±Ù†Ø¯

2. **13-multiplier formula** Ø¯Ø± final scoring
   - Ø§ÛŒÙ† ÙØ±Ù…ÙˆÙ„ Ø§ØµÙ„ Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª

3. **Correlation calculation method** (np.corrcoef)
   - Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¢Ù…Ø§Ø±ÛŒ

4. **Timeframe list** (`['5m', '15m', '1h', '4h']`)
   - ØªØºÛŒÛŒØ± Ø§ÛŒÙ† Ù„ÛŒØ³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¯Ø§Ø±Ø¯

5. **Ù…Ù†Ø·Ù‚ MACD Market Types** (A, B, C, D, X)
   - Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù¾Ø§ÛŒÙ‡

6. **Circuit breaker reset period** (24 hours)
   - Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡

---

## 5ï¸âƒ£ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Calibration

### 5.1 Grid Search

**Ø¨Ø±Ø§ÛŒ:** Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ ÙØ¶Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©ÙˆÚ†Ú©

```python
from itertools import product

# Define parameter ranges
param_grid = {
    'slope_5m': [0.12, 0.15, 0.18],
    'slope_1h': [0.08, 0.10, 0.12],
    'direction_margin': [1.2, 1.3, 1.4],
    'min_score': [170, 180, 190]
}

# Generate all combinations
combinations = list(product(*param_grid.values()))

# Test each combination
best_params = None
best_sharpe = -999

for combo in combinations:
    params = dict(zip(param_grid.keys(), combo))

    # Run backtest
    results = backtest(params)

    # Evaluate
    if results['sharpe'] > best_sharpe:
        best_sharpe = results['sharpe']
        best_params = params

print(f"Best params: {best_params}")
print(f"Best Sharpe: {best_sharpe}")
```

**âš ï¸ Ø®Ø·Ø±:** Combinatorial explosion - ØªØ¹Ø¯Ø§Ø¯ combinations Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

---

### 5.2 Random Search

**Ø¨Ø±Ø§ÛŒ:** Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ ÙØ¶Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø²Ø±Ú¯

```python
import random

# Define parameter ranges
param_ranges = {
    'slope_5m': (0.10, 0.20),
    'slope_15m': (0.08, 0.16),
    'slope_1h': (0.06, 0.14),
    'slope_4h': (0.05, 0.12),
    'direction_margin': (1.1, 1.6),
    'min_score': (140, 220),
    # ... more params
}

# Random sampling
n_iterations = 500
best_params = None
best_metric = -999

for i in range(n_iterations):
    # Sample random parameters
    params = {
        name: random.uniform(*range_)
        for name, range_ in param_ranges.items()
    }

    # Backtest
    results = backtest(params)

    # Track best
    if results['sharpe'] > best_metric:
        best_metric = results['sharpe']
        best_params = params

    if i % 50 == 0:
        print(f"Iteration {i}/{n_iterations}, Best Sharpe: {best_metric:.3f}")

print(f"Best params found: {best_params}")
```

**Ù…Ø²ÛŒØª:** Ú©Ø§Ø±Ø¢Ù…Ø¯ØªØ± Ø§Ø² grid search Ø¨Ø±Ø§ÛŒ ÙØ¶Ø§ÛŒ Ø¨Ø²Ø±Ú¯.

---

### 5.3 Bayesian Optimization

**Ø¨Ø±Ø§ÛŒ:** Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ (recommended)

```python
from skopt import gp_minimize
from skopt.space import Real, Integer

# Define search space
space = [
    Real(0.10, 0.20, name='slope_5m'),
    Real(0.08, 0.16, name='slope_15m'),
    Real(0.06, 0.14, name='slope_1h'),
    Real(0.05, 0.12, name='slope_4h'),
    Real(1.1, 1.6, name='direction_margin'),
    Integer(140, 220, name='min_score'),
]

# Objective function
def objective(params):
    param_dict = {
        'slope_5m': params[0],
        'slope_15m': params[1],
        'slope_1h': params[2],
        'slope_4h': params[3],
        'direction_margin': params[4],
        'min_score': params[5],
    }

    # Run backtest
    results = backtest(param_dict)

    # Return negative Sharpe (minimize)
    return -results['sharpe']

# Optimize
result = gp_minimize(
    objective,
    space,
    n_calls=100,  # number of evaluations
    random_state=42,
    verbose=True
)

print(f"Best params: {result.x}")
print(f"Best Sharpe: {-result.fun}")
```

**Ù…Ø²ÛŒØª:** Intelligent search - ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ø¯Ø§Ù… Ù†ÙˆØ§Ø­ÛŒ Ø¨Ù‡ØªØ± Ù‡Ø³ØªÙ†Ø¯.

---

### 5.4 Genetic Algorithm

**Ø¨Ø±Ø§ÛŒ:** Optimization Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø§ constraint Ù‡Ø§

```python
from deap import base, creator, tools, algorithms
import random

# Define fitness and individual
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

# Individual: [slope_5m, slope_15m, slope_1h, slope_4h, margin, min_score]
def create_individual():
    return [
        random.uniform(0.10, 0.20),  # slope_5m
        random.uniform(0.08, 0.16),  # slope_15m
        random.uniform(0.06, 0.14),  # slope_1h
        random.uniform(0.05, 0.12),  # slope_4h
        random.uniform(1.1, 1.6),    # direction_margin
        random.randint(140, 220)     # min_score
    ]

# Evaluation function
def evaluate(individual):
    params = {
        'slope_5m': individual[0],
        'slope_15m': individual[1],
        'slope_1h': individual[2],
        'slope_4h': individual[3],
        'direction_margin': individual[4],
        'min_score': individual[5],
    }

    results = backtest(params)
    return (results['sharpe'],)  # tuple

# Setup genetic algorithm
toolbox = base.Toolbox()
toolbox.register("individual", tools.initIterate, creator.Individual, create_individual)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)
toolbox.register("evaluate", evaluate)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# Run
population = toolbox.population(n=50)
algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=40, verbose=True)

# Best individual
best = tools.selBest(population, k=1)[0]
print(f"Best params: {best}")
print(f"Best Sharpe: {best.fitness.values[0]}")
```

---

## 6ï¸âƒ£ Backtest Strategy

### 6.1 Data Split

```python
# CRITICAL: Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² over-fitting

# 1. Training Set (60%)
train_start = '2020-01-01'
train_end = '2022-12-31'

# 2. Validation Set (20%)
val_start = '2023-01-01'
val_end = '2023-08-31'

# 3. Test Set (20%)
test_start = '2023-09-01'
test_end = '2024-06-30'

# Process:
# 1. Optimize on Training Set
# 2. Validate on Validation Set
# 3. Final evaluation on Test Set (NEVER optimize on this!)
```

**âš ï¸ Ù‡Ø´Ø¯Ø§Ø± Ù…Ù‡Ù…:**
- **NEVER** optimize on test set
- Test set ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª
- Ø§Ú¯Ø± optimization Ø±ÙˆÛŒ test set Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯ = over-fitting

---

### 6.2 Cross-Validation

```python
# Time Series Cross-Validation
from sklearn.model_selection import TimeSeriesSplit

# 5-fold time series split
tscv = TimeSeriesSplit(n_splits=5)

results = []

for train_index, val_index in tscv.split(data):
    train_data = data.iloc[train_index]
    val_data = data.iloc[val_index]

    # Optimize on train
    best_params = optimize(train_data)

    # Evaluate on validation
    metrics = backtest(val_data, best_params)
    results.append(metrics)

# Average metrics across folds
avg_sharpe = np.mean([r['sharpe'] for r in results])
std_sharpe = np.std([r['sharpe'] for r in results])

print(f"Average Sharpe: {avg_sharpe:.3f} Â± {std_sharpe:.3f}")
```

---

### 6.3 Market Regime Separation

```python
# Ø¨Ù‡ØªØ± Ø§Ø³Øª optimization Ø±Ø§ Ø¯Ø± regime Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯

# Detect market regimes
regimes = detect_market_regimes(data)

# Separate data
trending_data = data[regimes == 'trending']
ranging_data = data[regimes == 'ranging']
volatile_data = data[regimes == 'volatile']

# Optimize separately
trending_params = optimize(trending_data)
ranging_params = optimize(ranging_data)
volatile_params = optimize(volatile_data)

# Create regime-adaptive config
config_adaptive = {
    'trending': trending_params,
    'ranging': ranging_params,
    'volatile': volatile_params
}
```

---

## 7ï¸âƒ£ Walk-Forward Analysis

**Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² over-fitting**

```python
# Walk-Forward Optimization
# Ù…Ø«Ø§Ù„: Optimize Ù‡Ø± 3 Ù…Ø§Ù‡ØŒ Test 1 Ù…Ø§Ù‡

optimization_window = 90  # days
test_window = 30  # days

all_results = []
param_history = []

start_date = '2020-01-01'
end_date = '2024-06-30'

current_date = start_date

while current_date < end_date:
    # 1. Optimization period
    opt_start = current_date
    opt_end = opt_start + timedelta(days=optimization_window)
    opt_data = data[opt_start:opt_end]

    # Optimize parameters
    best_params = optimize(opt_data)
    param_history.append({
        'date': current_date,
        'params': best_params
    })

    # 2. Test period (out-of-sample)
    test_start = opt_end
    test_end = test_start + timedelta(days=test_window)
    test_data = data[test_start:test_end]

    # Test with optimized params
    results = backtest(test_data, best_params)
    all_results.append(results)

    # Move forward
    current_date = test_end

# Aggregate results
total_return = np.sum([r['return'] for r in all_results])
avg_sharpe = np.mean([r['sharpe'] for r in all_results])

print(f"Walk-Forward Results:")
print(f"Total Return: {total_return:.2f}%")
print(f"Average Sharpe: {avg_sharpe:.3f}")

# Visualize parameter stability
plot_parameter_evolution(param_history)
```

**Ù†Ú©ØªÙ‡ Ù…Ù‡Ù…:** Ø§Ú¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ§Ø¯ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ = over-fitting

---

## 8ï¸âƒ£ Optimization Tips

### 8.1 âœ… Do's

1. **Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø² validation set Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯**
   ```python
   # GOOD
   params = optimize(train_data)
   results = evaluate(val_data, params)
   ```

2. **Metric Ù‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯**
   ```python
   metrics = {
       'sharpe_ratio': ...,
       'profit_factor': ...,
       'max_drawdown': ...,
       'win_rate': ...,
       'avg_trade': ...,
       'recovery_factor': ...,
   }
   ```

3. **Ø¯Ø± market regime Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ØªØ³Øª Ú©Ù†ÛŒØ¯**
   ```python
   # Bull market
   # Bear market
   # Ranging market
   # High volatility
   # Low volatility
   ```

4. **Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯**
   ```python
   # Ø§Ú¯Ø± ØªØºÛŒÛŒØ± Ú©ÙˆÚ†Ú© Ø¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ± ØªØ£Ø«ÛŒØ± Ø¨Ø²Ø±Ú¯ Ø¯Ø§Ø±Ø¯ = over-fitting

   # Test sensitivity
   for delta in [-10%, -5%, 0%, +5%, +10%]:
       param_perturbed = base_param * (1 + delta)
       results = backtest(param_perturbed)
   ```

5. **Ø§Ø² Occam's Razor Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯**
   ```
   "Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±Ø§Ù‡ Ø­Ù„ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ø³Øª"

   # Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ø¨Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø´Ø§Ø¨Ù‡ØŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
   ```

---

### 8.2 âŒ Don'ts

1. **Ø±ÙˆÛŒ test set optimize Ù†Ú©Ù†ÛŒØ¯**
   ```python
   # BAD - NEVER DO THIS
   params = optimize(test_data)
   ```

2. **ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ parameter Ø±Ø§ Ù‡Ù…Ø²Ù…Ø§Ù† optimize Ù†Ú©Ù†ÛŒØ¯**
   ```python
   # BAD - Too many parameters
   optimize_simultaneously([
       'slope_5m', 'slope_15m', 'slope_1h', 'slope_4h',
       'momentum_5m', 'momentum_15m', 'momentum_1h', 'momentum_4h',
       'pattern_scores_5m', 'pattern_scores_15m', ...
       # 50+ parameters!
   ])

   # GOOD - Optimize in stages
   # Stage 1: Slope thresholds
   # Stage 2: Momentum thresholds
   # Stage 3: Pattern scores
   ```

3. **ÙÙ‚Ø· Ø±ÙˆÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ optimize Ù†Ú©Ù†ÛŒØ¯**
   ```python
   # BAD - Over-fit to BTC
   params = optimize(btc_data)

   # GOOD - Optimize on multiple symbols
   params = optimize([btc_data, eth_data, bnb_data, ...])
   ```

4. **ÙÙ‚Ø· Ø¨Ù‡ Sharpe Ù†Ú¯Ø§Ù‡ Ù†Ú©Ù†ÛŒØ¯**
   ```python
   # BAD - Single metric
   best = max(results, key=lambda x: x['sharpe'])

   # GOOD - Multi-objective
   best = find_pareto_optimal(results, metrics=['sharpe', 'max_dd', 'win_rate'])
   ```

5. **ÙØ±Ø§Ù…ÙˆØ´ Ù†Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯**
   ```python
   # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ 2020 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± 2024 Ú©Ø§Ø± Ù†Ú©Ù†Ù†Ø¯
   # Ù†ÛŒØ§Ø² Ø¨Ù‡ re-optimization Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±ÛŒØ¯
   ```

---

## 9ï¸âƒ£ Practical Example - Complete Workflow

```python
#!/usr/bin/env python3
"""
Complete Backtest Calibration Workflow
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from skopt import gp_minimize
from skopt.space import Real, Integer
import yaml

# ============================================================
# 1. Load Data
# ============================================================

def load_data(symbols, start, end, timeframe='1h'):
    """Load historical OHLCV data"""
    data = {}
    for symbol in symbols:
        df = fetch_ohlcv(symbol, timeframe, start, end)
        data[symbol] = df
    return data

# ============================================================
# 2. Split Data
# ============================================================

def split_data(data, train_ratio=0.6, val_ratio=0.2):
    """Split into train/val/test sets"""
    total_len = len(data)
    train_end = int(total_len * train_ratio)
    val_end = int(total_len * (train_ratio + val_ratio))

    return {
        'train': data[:train_end],
        'val': data[train_end:val_end],
        'test': data[val_end:]
    }

# ============================================================
# 3. Define Optimization Space
# ============================================================

# Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Priority 1
PARAM_SPACE = [
    # Slope thresholds
    Real(0.12, 0.20, name='slope_5m'),
    Real(0.10, 0.16, name='slope_15m'),
    Real(0.08, 0.14, name='slope_1h'),
    Real(0.05, 0.12, name='slope_4h'),

    # Direction margin
    Real(1.1, 1.6, name='direction_margin'),

    # Minimum signal score
    Integer(150, 210, name='min_signal_score'),

    # Timeframe weights
    Real(0.6, 0.9, name='weight_5m'),
    Real(0.8, 1.0, name='weight_15m'),
    Real(1.0, 1.3, name='weight_4h'),

    # Risk parameters
    Real(1.5, 3.0, name='default_sl_percent'),
    Real(1.5, 3.0, name='preferred_rr_ratio'),
    Real(1.5, 2.5, name='atr_multiplier'),
]

# ============================================================
# 4. Backtest Function
# ============================================================

def run_backtest(data, params):
    """Run backtest with given parameters"""

    # Create config from params
    config = {
        'signal_generation': {
            'minimum_signal_score': params['min_signal_score'],
            'timeframe_weights': {
                '5m': params['weight_5m'],
                '15m': params['weight_15m'],
                '1h': 1.0,
                '4h': params['weight_4h']
            },
            'trend_detection': {
                'slope_thresholds': {
                    '5m': params['slope_5m'],
                    '15m': params['slope_15m'],
                    '1h': params['slope_1h'],
                    '4h': params['slope_4h']
                }
            }
        },
        'multi_timeframe': {
            'direction_margin': params['direction_margin']
        },
        'risk': {
            'default_stop_loss_percent': params['default_sl_percent'],
            'preferred_risk_reward_ratio': params['preferred_rr_ratio'],
            'atr_trailing_multiplier': params['atr_multiplier']
        }
    }

    # Initialize backtest engine
    from backtest.backtest_engine_v2 import BacktestEngineV2
    engine = BacktestEngineV2(config)

    # Run backtest
    results = engine.run(data)

    return results

# ============================================================
# 5. Objective Function
# ============================================================

def objective(params_list):
    """Objective function to minimize (negative Sharpe)"""

    # Convert params list to dict
    params = {
        'slope_5m': params_list[0],
        'slope_15m': params_list[1],
        'slope_1h': params_list[2],
        'slope_4h': params_list[3],
        'direction_margin': params_list[4],
        'min_signal_score': params_list[5],
        'weight_5m': params_list[6],
        'weight_15m': params_list[7],
        'weight_4h': params_list[8],
        'default_sl_percent': params_list[9],
        'preferred_rr_ratio': params_list[10],
        'atr_multiplier': params_list[11],
    }

    # Run backtest on training data
    results = run_backtest(train_data, params)

    # Calculate composite score
    sharpe = results['sharpe_ratio']
    max_dd = results['max_drawdown']
    profit_factor = results['profit_factor']

    # Multi-objective: Maximize Sharpe, Minimize DD, Maximize PF
    score = sharpe - (max_dd / 10) + (profit_factor / 2)

    # Return negative (for minimization)
    return -score

# ============================================================
# 6. Main Optimization Loop
# ============================================================

def main():
    print("=" * 60)
    print("BACKTEST CALIBRATION - Priority 1 Parameters")
    print("=" * 60)

    # Load data
    print("\n1. Loading data...")
    symbols = ['BTC/USDT', 'ETH/USDT', 'BNB/USDT']
    data = load_data(symbols, start='2020-01-01', end='2024-06-30')

    # Split data
    print("2. Splitting data...")
    global train_data, val_data, test_data
    splits = split_data(data)
    train_data = splits['train']
    val_data = splits['val']
    test_data = splits['test']

    print(f"   Train: {len(train_data)} days")
    print(f"   Val:   {len(val_data)} days")
    print(f"   Test:  {len(test_data)} days")

    # Optimize on training data
    print("\n3. Optimizing parameters...")
    print("   (This may take several hours...)")

    result = gp_minimize(
        objective,
        PARAM_SPACE,
        n_calls=100,  # 100 iterations
        random_state=42,
        verbose=True,
        n_jobs=-1  # Use all CPU cores
    )

    # Extract best parameters
    best_params = {
        'slope_5m': result.x[0],
        'slope_15m': result.x[1],
        'slope_1h': result.x[2],
        'slope_4h': result.x[3],
        'direction_margin': result.x[4],
        'min_signal_score': result.x[5],
        'weight_5m': result.x[6],
        'weight_15m': result.x[7],
        'weight_4h': result.x[8],
        'default_sl_percent': result.x[9],
        'preferred_rr_ratio': result.x[10],
        'atr_multiplier': result.x[11],
    }

    print("\n4. Best parameters found:")
    for key, value in best_params.items():
        print(f"   {key}: {value:.4f}")

    # Validate on validation set
    print("\n5. Validating on validation set...")
    val_results = run_backtest(val_data, best_params)

    print(f"   Sharpe Ratio: {val_results['sharpe_ratio']:.3f}")
    print(f"   Max Drawdown: {val_results['max_drawdown']:.2f}%")
    print(f"   Profit Factor: {val_results['profit_factor']:.3f}")
    print(f"   Win Rate: {val_results['win_rate']:.2f}%")

    # Final test on test set
    print("\n6. Final test on test set...")
    test_results = run_backtest(test_data, best_params)

    print(f"   Sharpe Ratio: {test_results['sharpe_ratio']:.3f}")
    print(f"   Max Drawdown: {test_results['max_drawdown']:.2f}%")
    print(f"   Profit Factor: {test_results['profit_factor']:.3f}")
    print(f"   Win Rate: {test_results['win_rate']:.2f}%")

    # Save optimized config
    print("\n7. Saving optimized config...")
    save_config(best_params, 'config_optimized.yaml')

    print("\nâœ… Calibration complete!")
    print("=" * 60)

if __name__ == '__main__':
    main()
```

---

## ğŸ”Ÿ Summary & Priority Roadmap

### Phase 1: Quick Wins (Week 1-2)

**Focus:** Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ Ø¨Ø§ ØªØ£Ø«ÛŒØ± Ø¨Ø§Ù„Ø§

1. âœ… **Direction Margin** - ÛŒÚ© Ù¾Ø§Ø±Ø§Ù…ØªØ±ØŒ ØªØ£Ø«ÛŒØ± Ø²ÛŒØ§Ø¯
2. âœ… **Minimum Signal Score** - ÛŒÚ© Ù¾Ø§Ø±Ø§Ù…ØªØ±ØŒ ØªØ£Ø«ÛŒØ± Ø²ÛŒØ§Ø¯
3. âœ… **Risk/Reward Ratios** - 3 Ù¾Ø§Ø±Ø§Ù…ØªØ±ØŒ ØªØ£Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø± Ø³ÙˆØ¯

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ:** 2-5 Ø±ÙˆØ² (Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø³Ø±Ø¹Øª backtest)

---

### Phase 2: Core Optimization (Week 3-4)

**Focus:** Trend & Momentum thresholds

4. âœ… **Per-TF Slope Thresholds** - 4 Ù¾Ø§Ø±Ø§Ù…ØªØ±
5. âœ… **Timeframe Weights** - 4 Ù¾Ø§Ø±Ø§Ù…ØªØ±
6. âœ… **Momentum Thresholds** - Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ:** 1-2 Ù‡ÙØªÙ‡

---

### Phase 3: Fine-Tuning (Week 5-6)

**Focus:** Pattern scores & advanced

7. âœ… **Pattern Scores per TF**
8. âœ… **MACD Type Strengths**
9. âœ… **Phase Multipliers**
10. âœ… **Confidence Thresholds**

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ:** 1-2 Ù‡ÙØªÙ‡

---

### Phase 4: Protection Systems (Week 7-8)

**Focus:** Circuit breaker & correlation

11. âœ… **Circuit Breaker Thresholds**
12. âœ… **Correlation Thresholds**

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ:** 1 Ù‡ÙØªÙ‡

---

### Phase 5: Validation (Week 9-10)

13. âœ… **Walk-Forward Analysis**
14. âœ… **Cross-Validation**
15. âœ… **Regime-based Testing**

**Ø²Ù…Ø§Ù† ØªØ®Ù…ÛŒÙ†ÛŒ:** 2 Ù‡ÙØªÙ‡

---

## âš ï¸ Final Warnings

1. **Over-fitting is the enemy**
   - Ù‡Ù…ÛŒØ´Ù‡ validation set Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
   - Ù‡Ø±Ú¯Ø² Ø±ÙˆÛŒ test set optimize Ù†Ú©Ù†ÛŒØ¯
   - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¯Ø± regime Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ú©Ø§Ø± Ú©Ù†Ù†Ø¯

2. **Start simple**
   - Ø§Ø¨ØªØ¯Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Priority 1 Ø±Ø§ calibrate Ú©Ù†ÛŒØ¯
   - Ø¨Ø¹Ø¯ Ø¨Ù‡ Priority 2 Ùˆ 3 Ø¨Ø±ÙˆÛŒØ¯
   - Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø±Ø§ Ù‡Ù…Ø²Ù…Ø§Ù† optimize Ù†Ú©Ù†ÛŒØ¯

3. **Market changes**
   - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ù…Ø±ÙˆØ² Ù…Ù…Ú©Ù† Ø§Ø³Øª ÙØ±Ø¯Ø§ Ø¨Ù‡ÛŒÙ†Ù‡ Ù†Ø¨Ø§Ø´Ù†Ø¯
   - Ù†ÛŒØ§Ø² Ø¨Ù‡ re-calibration Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ù‡Ø± 3-6 Ù…Ø§Ù‡)

4. **Domain knowledge matters**
   - Ø§Ø² backtest Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ø§Ù…Ø§ Ù…Ù†Ø·Ù‚ Ø±Ø§ ÙØ±Ø§Ù…ÙˆØ´ Ù†Ú©Ù†ÛŒØ¯
   - Ø§Ú¯Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ù…ÛŒâ€ŒØ±Ø³Ø¯ØŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ over-fit Ø§Ø³Øª

5. **Document everything**
   - Ù‡Ù…Ù‡ optimization Ù‡Ø§ Ø±Ø§ document Ú©Ù†ÛŒØ¯
   - Ø¯Ù„Ø§ÛŒÙ„ ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ ÛŒØ§Ø¯Ø¯Ø§Ø´Øª Ú©Ù†ÛŒØ¯
   - Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†ÛŒØ¯

---

**ğŸ“… Document Version:** 1.0
**ğŸ—“ï¸ Last Updated:** 2025-11-21
**âœï¸ Author:** Claude (AI Analysis)

**Good luck with your calibration! ğŸš€ğŸ“ˆ**
